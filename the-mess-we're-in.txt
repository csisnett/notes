
The Mess We're in by Joe Armstrong

https://www.youtube.com/watch?v=lKXe3HUG2l4

The problem: Computers today (2014) are perhaps 1000's of times better/faster than personal computers in their beginnings. But this increase in performance, better memory, etc hasn't really materialized as programs complexity have increased at the same or maybe even larger rate..

In the beginning: programmers would be able to have see exactly how their program would execute on the machine as they were close to the machine and this smallness also allowed them to hold the program in mind

Thesis: something went wrong in programming, as no matter how better the hardware got, the software kept performing at a similar rate

State of the world now: Because of this increasing complexity in programming, now we can't know exactly why programs don't work, we can't just "look at it" we've got layers over layers of stuff, and each program and the computer that runs it has so many possible states which makes it even harder to know what the cause is.


7 deadly sins:

1. Code even you cannot understand after you wrote it - no comments
2. Code with no specifications
3. Code that is shipped as soon as it runs and before it is beautiful
4. Code with added features
5. Code that is very very fast very very very obscure and incorrect
6. Code that is not beautiful
7. Code that you wrote without understanding the problem


What is legacy code(described jokingly):

- programmers who wrote the code are dead
- No specification
- Written in archaic languages which nobody understands
- "it works"
- Management thinks modifying legacy code is cheaper than a total re-write

How complex are current programs?

"So a C program with 6 32-bit integers can have more states than the number of atoms on the planet"

This means it's utterly impossible to test a program by trying all states and see if any of these work, and even more for real programs that deal with a LOT more data. fun fact: 3 variables in javascript have more states than the total of all atoms in the planet

So: program + environment_01332(OS) => works
but also: program + environment_01331(OS) => doesn't work

This is the reason why when we stackoverflow things that work for other people don't work for us! because our computers have different states. and running the same program in different states produces different results, how many different states can my computer have? well, I take a look at the math: https://youtu.be/lKXe3HUG2l4?t=906 :S

*At this point I'm wondering how anything works*

To handle failures we need at least two computers, if we have just one computer and our programs fails, it's dead.

The more computers we have in a distributed systems the lower the odds that they all fail at the same time.

"systems should self-heal, reconfigure and evolve over time"


We don't have common programming languages to speak with other programmers, "no lingua franca" no common build tool either

Efficiency and clarity

dichotomy:

To make something clearer add a layer of abstraction
To make something more efficient remove a layer of abstraction

Causality

a cause must always precede its event in order to have causality

some facts from physics:

information travels at less or at the speed of light
We do not know an event happened until we get a message saying the event happened
We do not know how things are at a remote location only how they were the last time we got a message from that location

Simultanuity https://youtu.be/lKXe3HUG2l4?t=1526

                     A                 B
A happened first   A and B happened at the same time     B happened first
Continue notes on: https://youtu.be/lKXe3HUG2l4?t=993

Speed of computation

By taking physics physics known limits we can calculate sort of the maximum speed of computation

This computation would be executed by the "ultimate laptop"

1 kg blackhole computer could do:

10^51 operations/per second

As things can't easily scape a blackhole the way to get information from this computer
would be to have two equal particles one goes in and other doesn't, and when the
state of the particle in the blackhole changes through quantum entanglement the
one on the outside changes its state and that one we can track its speed or whatever
property to know what the result of that computation was.

http://arxiv.org/abs/quant-ph/9908043

What can we do?

The problem with current programs is that they increase entropy.
A condenser could reduce the number of files, because many files are essentialy the same.

One way to condense is by abolishing names and places, take a string of something crypto:sha1
and you basically reduced it.

URIs are bad but.. 

hash://5849293sdgsf90saf9sa94904af

address cannot be spoofed (there is no address)
no problem choosing a name
can be cached forever
content can be validated on arrival
not subject to person-in-the-middle attacks
you can check the sum of the original file to make sure it wasn't tampered with in the trajectory.

Chord, Kademlia algorithms can help you with knowing on which machine that is

With this idea of hashing we could know like what files are similar in the whole world,
and start condensing them so that we don't have this unnecessary complexity

Also with a compression algorithm we could know if two files are similar.

highlights: 

https://youtu.be/lKXe3HUG2l4?t=494

